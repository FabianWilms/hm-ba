\documentclass[12pt,a4paper,bibliography=totocnumbered,listof=totocnumbered]{scrartcl}

\input{settings/packages}
\input{settings/commands}

\begin{document}

\input{frontpage}

% ----------------------------------------------------------------------------------------------------------
% Abstract
% ----------------------------------------------------------------------------------------------------------
\setcounter{page}{1}
\onehalfspacing
\titlespacing{\section}{0pt}{12pt plus 4pt minus 2pt}{2pt plus 2pt minus 2pt}
\rhead{KURZFASSUNG}
\section{Kurzfassung}

Ziel dieser Arbeit ist es, durch generative Testerstellung die Qualität von Microservice-Anwendungen zu steigern, ohne dabei Kosten und Zeitaufwand des Projekts zu erhöhen. Dazu werden zunächst spezifische Vorgehensweisen zum Testen von Microservice-Architekturen erläutert. Im nächsten Schritt werden diese dann in einem Referenzsystem implementiert. Bei der Implementierung wird Rücksicht auf die Generierbarkeit aller Tests genommen, sodass diese in die bestehende Infrastruktur zur Generierung von Microservice-Architekturen integriert werden können. Durch die Arbeit hat sich gezeigt, dass der generative Ansatz teilweise die angestrebte Qualitätssteigerung ermöglicht, wenn auch einige Einschränkungen in der Bandbreite der generierten Tests zu erkennen ist.

\vspace{-1,2em}
\titlespacing{\section}{0pt}{12pt plus 4pt minus 2pt}{-6pt plus 2pt minus 2pt}
\section*{Abstract}

This thesis aims to enhance the quality of a application in microservice-architecture without increasing time and cost expenses of the project by generating tests. To validate this claim specific methods are explained which are especially useful for testing microservice architectures. These methods then get implemented while taking into concern that these tests shall be generated at a later point. This thesis has shown that the generation of these tests partly fulfills the desired rise in quality even though there are recognizable limitations to the spectrum in which the generated tests can operate.

\pagebreak

\input{inhaltsverzeichnis}

% ----------------------------------------------------------------------------------------------------------
% Inhalt
% ----------------------------------------------------------------------------------------------------------
% Abstände Überschrift
\titlespacing{\section}{0pt}{12pt plus 4pt minus 2pt}{-6pt plus 2pt minus 2pt}
\titlespacing{\subsection}{0pt}{12pt plus 4pt minus 2pt}{-6pt plus 2pt minus 2pt}
\titlespacing{\subsubsection}{0pt}{12pt plus 4pt minus 2pt}{-6pt plus 2pt minus 2pt}

% Kopfzeile
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\subsectionmark}[1]{}
\renewcommand{\subsubsectionmark}[1]{}
\lhead{Kapitel \thesection}
\rhead{\rightmark}

\onehalfspacing
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\theHsection}{\arabic{section}}
\setcounter{section}{0}
\pagenumbering{arabic}
\setcounter{page}{1}
\setcounter{secnumdepth}{6}

% ----------------------------------------------------------------------------------------------------------
% Einführung und Motivation
% ----------------------------------------------------------------------------------------------------------
\section{Einführung und Motivation}

Ziel dieser Arbeit ist es, mithilfe von generativer Testerstellung die Qualität einer Anwendung in Microservice-Architektur steigen zu lassen, ohne dabei die Kosten des Projekts zu erhöhen.

Sowohl im privaten als auch geschäftlichen Alltag nimmt IT eine immer größere Rolle ein. Die Übernahme von Bereichen, die ehemals als nicht durch Computer austauschbar erachtet wurden, schreitet immer weiter fort. Doch dadurch steigen nicht nur bestehende Anforderungen an Software, sondern es entstehen auch neue Kriterien an die Qualität. Sobald Türklingeln, Alarmanlagen und Schließanlagen \textit{smart} werden, ist die Fehlertoleranz gleich null. Auch Autos und medizinische Geräte sind nicht ausgenommen. In diesen Bereichen kann es sogar um Menschenleben gehen. Ganz abgesehen davon steigt auch die Komplexität von modernen Software-Systemen immens an, auch weil dies von den Nutzern der Anwendungen durch den Wunsch an neuen Features gefordert wird.\cite{pantesting}.

Mit steigender Komplexität und höherer Nachfrage am Markt, sowie engen Zeitplänen für Projekte wird leider häufig aus Zeit- und Kostengründen auf Qualität nur geringfügig Rücksicht genommen. Zunächst verursacht eine gute Software-Qualität nämlich Mehrkosten. Personelle wie zeitliche. Dies zeigt das Magische Dreieck, oder im englischen das Project Management Triangle(siehe Abb.~\ref{fig:images/img_magisches_dreieck.eps}).

\image[0.4]{images/img_magisches_dreieck.eps}{Magisches Dreieck des Projektmanagements \cite{hagen}}

Dieses besagt, dass die Qualität eines Projekts durch die drei Faktoren Leistung, Kosten und Zeit beeinflusst wird. Diese Faktoren müssen vom Management eines Projekts möglichst ausbalanciert gehalten werden. Beispiel anhand des Hausbaus: Vom Bauherren ist ein fester Termin für die Fertigstellung des Hauses angedacht (Faktor Zeit), doch lässt der aktuelle Baufortschritt eine Fertigstellung zum festgelegten Termin nicht mehr zu. Die Lösung wäre, mehr Arbeiter einzustellen und somit den Fortschritt zu beschleunigen (erhöhung der Kosten), oder die Arbeiten am Haus schneller und mit geringerer Qualitätsanforderung durchführen zu lassen (senkung der Qualität). Gleiches gilt auch für die Softwareentwicklung.

Zu beachten ist jedoch, dass eine hohe Qualität von Beginn des Projekts an angestrebt werden sollte, und nicht erst dann, wenn alle anderen Aufgaben erledigt sind. Ein Bericht der Kölner Beratungsfirma SQS zeigt anhand von gesammelten Zahlen aus Beratungsaufträgen welche immensen Kosten durch unentdeckte Fehler entstehen \cite{sqsdefect}. Hier wird besonders deutlich wie wichtig es für ein Projekt ist, frühzeitige Qua\-li\-täts\-si\-che\-rung durchzusetzen. Und dazu zählt auch das Testen von Software.

\image{images/img_sqs-defect-correction.PNG}{SQS Report Costs of Defect Correction \cite{sqsdefect}}

Umso früher Fehler entdeckt und bemerkt werden, umso weniger kostet es auch diese zu beheben. Wenn bereits vor dem Start der Implementierungsphase auf eine hohe Testabdeckung, beispielsweise durch den Einsatz von test-driven development, wert gelegt wird, können, je auftretendem Fehler, um die 7800\euro\ (siehe Abb.~\ref{fig:images/img_sqs-defect-correction.PNG}) eingespart werden. Mit diesen Zahlen sind die Mehrkosten, die für ein solches vorgehen entstehen um ein vielfaches leichter zu rechtfertigen.

Somit sorgt das Bug-Fixing in Produktivsystemen, also das Beheben sogenannter \textit{field defects}, für einen der größten Kostenfaktoren. Wurde in den ersten Phasen eines Projekts nicht viel, oder kein Wert auf eine ausreichende Test-Abdeckung gelegt schaffen es viele Fehler in die Produktivsysteme der Hersteller. Doch werden diese Fehler erst im laufenden Betrieb beim Kunden festgestellt, ist es bereits zu spät. Robert N. Charette kritisiert eben dies in seinem Artikel \textit{Why Software Fails}\cite{charette}.

\myquote{If the software coders don't catch their omission until final system testing--or worse, until after the system has been rolled out--the costs incurred to correct the error will likely be many times greater than if they'd caught the mistake while they were still working on the initial [...] process.}

Die Lösung sollte also sein, viel Zeit und Geld in gute Softwarequalität zu investieren. Jedoch stehen, wie bereits am Anfang der Einleitung erwähnt, Projektleiter und ihre Mitarbeiter unter hohem zeitlichen Druck, vom Kunden oder durch andere beteiligte, festgelegte Termine einzuhalten. Hinzu kommt, dass gute Tests zur Steigerung der Qualität neben Geld auch Zeit kosten (siehe dazu Abb~\ref{fig:images/img_magisches_dreieck.eps}). Es entsteht in dieser Zeit aber kein Fortschritt an der Funktionalität der Software.

Im Kommunalen Umfeld sind die zeitlichen Restriktionen noch einmal stärker zu gewichten als in der freien Wirtschaft. Viele Projekte werden aufgrund von anstehenden Gesetzesänderungen ins Leben gerufen und müssen mit Inkrafttreten der neuen Regelungen in Produktion gehen. it@M ist somit ständig auf der Suche nach Lösungen, die den Entwicklungs- und Testprozess beschleunigen, um die geringe Zeit möglichst effizient nutzen zu können.

Eine dieser Lösungen wurde im letzten Jahr von Martin Kurz im Rahmen seiner Masterarbeit \cite{mkthesis} geplant und entwickelt. Die Model-driven Software Development Lösung Barrakuda\footnote{Github Repository von Barrakuda (https://github.com/xdoo/mdsd)}. Diese bietet den Entwicklern von it@M die Möglichkeit anhand von einer vorgegebenen Domänen-spezifischen Sprache Microservice-Architekturen zu modellieren und diese zu generieren.

Im Rahmen dieser Arbeit soll durch die Entwicklung von Testmethoden für Microservice-Architekturen der Grundstein für eine Erweiterung von Barrakuda gelegt werden. Diese Weiterentwicklung soll einen Großteil verschiedener Testmethoden für diese Architektur generieren und die benötigte Entwicklungszeit für eine hohe Testabdeckung möglichst stark reduzieren.

Zunächst werden die Microservice Architektur (Kapitel~\ref{ch:ms-arch}) und der Monolithischen Architekturstil (Kapitel~\ref{ch:mon-arch}) beschrieben und bezüglich ihrer Vor- (Kapitel~\ref{ch:ms-mon-pro}) und Nachteile (Kapitel~\ref{ch:ms-mon-cons}) verglichen. Anschließend wird als Vorbereitung auf die darauf folgenden Kapitel die vorgegebene Architektur von it@M erläutert und auch die interne Struktur der Komponenten dargestellt (Kapitel~\ref{ch:arch-itm}).

Im nächsten Schritt werden bekannte Methoden zum Testen von Software festgestellt und analysiert (Kapitel~\ref{ch:soft-test}). Dann werden, anhand der im vorherigen Kapitel festgestellten Eigenschaften von Microservice Architekturen, Test-Methoden, die besonders im Bereich dieses Architekturmusters sinnvoll zu verwenden sind, weiter ausgeführt und aufgestellt (Kapitel~\ref{ch:ms-test}).

Abschließend werden diese Testmethoden im Kapitel der Implementierung (Kapitel~\ref{ch:implementierung}) in einem Referenz-System, unter ständiger Beachtung der möglichen Generierbarkeit aller Tests, implementiert und deren Effektivität und die mögliche Zeitersparnis analysiert. Am Ende wird im Fazit eine Beurteilung über die Sinnhaftigkeit und den möglichen Mehrwert zur Generierung der Tests aufgestellt (Kapitel~\ref{ch:fazit}).

% ----------------------------------------------------------------------------------------------------------
% Architektur-Generierung für Software-Projekte
% ----------------------------------------------------------------------------------------------------------
\section{Architekturvergleich Microservice und Monolith}

Eine der momentan am meisten diskutierten Architekturen ist die \enquote{Microservice} Architektur\footnote{\url{https://www.google.de/trends/explore?q=microservices} oder \url{https://jaxenter.de/ausgaben/java-magazin-5-16} oder \url{https://www.heise.de/developer/artikel/Microservices-im-Zusammenspiel-mit-Continuous-Delivery-Teil-1-die-Theorie-2376386.html?artikelseite=2}}: Viele kleine Services die im Konglomerat für ein gemeinsames Ziel zusammenarbeiten. Dabei gibt es insbesondere in der Kommunikation zwischen den Komponenten Unterschiede zwischen monolithischen und Microservice Architekturen. Während diese Komponenten in Monolithen gebündelt in einem Anwendungskontext agieren, passiert dies in Microservice-Systemen verteilt (vgl. Abb.~\ref{fig:images/img_monolith-vs-ms.eps}).

\image[0.8]{images/img_monolith-vs-ms.eps}{Logische Unterscheidung zwischen monolithischer und Microservice Architektur}

Sam Newman stellt in seinem Buch \textit{Building Microservices}\cite{buildingms} einige Vorteile dar, die Microservices gegenüber monolithischen Architekturen bieten. Doch um diese Vorteile besser zu verstehen muss man zunächst die Unterschiede der zwei Architekturstile im Detail betrachten.

\subsection{Microservice Architektur}\label{ch:ms-arch}

Microservices sind eine Art der Architektur, in der die Modularisierung im Vordergrund steht. Dazu werden komplexe Anwendungen in kleinere, einzelne Prozesse aufgeteilt die miteinander Sprachunabhängig in Verbindung stehen. \cite[S. 2-3]{wolff}

Um die Vorteile, die diese Architektur mit sich bringt, in einem System effektiv zu nutzen gibt es bereits in der Planung einiges zu beachten.

Dazu zählt, dass es zwei Dinge gibt, die einen guten Microservice ausmachen. Lose Kopplung und Starker Zusammenhalt \cite[S.62]{buildingms}.

Lose Kopplung bedeutet, dass Änderungen an einem Service keine Änderungen an einem anderen Service nach sich ziehen. Ist dies nicht gegeben, ist einer der Hauptvorteile von dieser Art Architektur nicht mehr vorhanden.
Ein lose gekoppelter Service weiß von seinem Kommunikationspartner nur so wenig wie möglich.\cite[S.63]{buildingms}

Starker Zusammenhalt soll dafür sorgen, dass bestimmte Funktionalitäten an einem Ort vorhanden sind, sodass diese leicht geändert werden können und nicht mehrere Komponenten angepasst werden müssen.\cite[S.64]{buildingms}

\subsubsection{Definition von Service-Grenzen}

Zum Planen einer Software mit Mircoservice Architektur ist es von großer Bedeutung, sich zunächst über die Grenzen seiner Services Gedanken zu machen. Als Ansatz zur Planung gibt es zwei bekannte Möglichkeiten.

\paragraph{Bounded Context} Kontextgrenzen sind eine Definition aus dem Buch \textit{Domain Driven Design} von Eric Evans \cite{dddesign} und bieten eine Möglichkeit in der Planungsphase eines Projekts diese Grenzen zu definieren.

Dies lässt sich gut am Beispiel eines Online-Shops zeigen. Wenn ein Online-Shop geplant wird ist einer der zentralen Bestandteile die Lagerverwaltung. Die Daten der Lagerverwaltung werden herangezogen um dem potentiellen Kunden des Shops anzuzeigen welche und wie viele Produkte verfügbar sind. Doch möchte ein Kunde nun ein Produkt bestellen, hat dies nichts mehr mit einer Lagerverwaltung, sondern mit einem Bestellsystem zu tun. Man verlässt also den ursprünglichen Kontext der Lagerverwaltung. Somit entsteht in der Planung eine neue Kontextgrenze: Die des Bestellsystems. Es verwaltet Bestellungen von Kunden. Doch woher kommen die Kundendaten? Kundendaten haben zwar einen Verwendungszweck im Bestellsystem, doch die Verwaltung der Daten hat mit der eigentlichen Kompetenz dieses Systems nichts mehr zu tun. Somit wird eine weitere Kontextgrenze erstellt, nämlich die der Kundenverwaltung(siehe Abb.~\ref{fig:images/img_online-shop-example.eps}).

\image[]{images/img_online-shop-example.eps}{Beispiel der Kontexgrenzen des Online-Shops}

Eine Kontextgrenze soll also eine logische Grenze darstellen, welche eventuell über eine oder mehrere Schnittstellen verfügt, die festlegen, welche Informationen mit anderen Kontexten geteilt werden.\cite[S.65]{buildingms}. Damit einher geht die Unterscheidung zwischen geteilten und versteckten Modellen. Versteckte Modelle werden innerhalb einer Kontextgrenze benötigt, sind aber für andere Kontexte uninteressant. Geteilte Modelle hingegen werden über die Grenzen hinweg freigegeben. Sind solche Kontextgrenzen für eine Software modelliert, lassen sich aus diesen sehr leicht Microservices ableiten, da bereits einige Grundvoraussetzungen getroffen sind: Lose Kopplung und Starker Zusammenhalt.\cite[S.68]{buildingms}

\myquote{[I]f our service boundaries align to the bounded contexts in our domain, and our microservices
	represent those bounded contexts, we are off to an excellent start in ensuring that our microservices
	are loosely coupled and strongly cohesive.\cite{buildingms}}

\paragraph{Aggregates}

Sind der zweite Mögliche Ansatz zur Planung von sinnvollen Service-Grenzen. Aggregates sind ein weiteres Konzept des \ac{DDD} und stellen eine Zusammensetzung aus Events und Objekten dar, die durch ein Aggregate Root zusammengehalten werden.

Als Beispiel dient hier ein Tisch der im Baumarkt gekauft wurde. Dieser Tisch besteht aus einer Sammlung von Holzstücken und Schrauben. Wenn diese auf einem Haufen liegen stellt dies noch lange keinen Tisch dar. Um den Tisch zu erhalten benötigt man eine Anleitung (die Events) die besagen, wie mit den Materialien umgegangen werden soll, um diesen Tisch zu erhalten. Diese Konstellation, von Materialien und der Anleitung zum Aufbau des Tisches, stellt ein Aggregate dar. Die Events alleine erfüllen keinen tieferen Zweck, auch die Materialien nicht. Aber vereint in einem Aggregate bilden sie den gewünschten Tisch.\cite{mogosanu}

Wenn man dieses Konzept nun auf das vorhergehende Online-Shop Beispiel überträgt, ergibt sich ein Bild wie in Abb.~\ref{fig:images/img_online-shop-example-aggregate.eps}.

\image[]{images/img_online-shop-example-aggregate.eps}{Aggregates im Online-Shop Beispiel}

Wie zu sehen ist, decken sich die gefundenen Aggregates mit dem im vorherigen Abschnitt gefundenen Kontextgrenzen. Somit ist das Ergebnis der Aufteilung auch mit dieser Methode das gleiche. Aggregates machen insbesondere dann Sinn, wenn mit Event-basierter Kommunikation gearbeitet wird, da die Zustandsänderungen, die durch Events beschrieben werden, der Darstellung von Bearbeitungsregeln von Aggregates sehr nahe kommen.

\subsubsection{Kommunikation zwischen Services}

Als weiterer Schritt muss eine Kommunikationsart für die Kommunikation gewählt werden. Dabei gibt es drei Mögliche Kommunikationswege:

\begin{itemize}
	\item Services kommunizieren Untereinander
	\item Kommunikation mit einem Service von außen
	\item Service kommuniziert mit einem anderen System
\end{itemize}

Wobei der letzte Punkt aufgrund der Abhängigkeit zum externen System vernachlässigt werden kann.

\paragraph{Geteilte Datenbanken} sind eine Möglichkeit zur Servicekommunikation \cite[S.85]{buildingms}. Die Idee hinter dieser Art der Kommunikation ist besonders trivial. Mehrere Services, die miteinander Daten austauschen wollen, nutzen eine gemeinsame Datenbank. Jeder Service hat somit jederzeit Zugriff zu allen Daten der anderen Nutzer dieser geteilten Datenbank und es findet somit keine \textit{echte} Kommunikation statt, sondern nur Zugriffe auf geteilten Speicher.\cite{shareddb}
Dies birgt natürlich viele Probleme. Jede noch so kleine Änderung an der "Logik" dieser Datenbank, oder an der internen Struktur der Daten muss mit viel Bedacht durchgeführt werden, da jede abhängige Komponente sonst nicht mehr funktionieren könnte.

Des Weiteren ist die technologische Einschränkung ein großer Nachteil. Wenn es auch in den ersten Schritten der Planung und Entwicklung Sinnvoll erscheint eine relationale Datenbank zu verwenden, können spätere Geschäftsentscheidungen oder neu auftretende Probleme den Einsatz einer Graphdatenbank sinnvoller machen. Bei einer geteilten Datenbank eine solche Änderung durchzuführen ist sehr schwierig\cite[S.85]{buildingms}.

\paragraph{\acfp{RPC}}\label{rpcpara} sind eine weitere bekannte Kommunikationsmöglichkeit. Die Übersetzung ins deutsche erklärt schon einen Großteil der Idee hinter \acp{RPC}: \enquote{Aufruf einer fernen Prozedur}. \acp{RPC} sind eine weitere Möglichkeit eine Client-Server-Modell umzusetzen. Die erste Idee dazu kam im Jahre 1976 von James White in seinem RFC \#707 \enquote{A High-Level Framework for Network-Based Resource Sharing}\cite{white707}. Ein \ac{RPC} funktioniert, indem der Client einer Anwendung eine Anfrage an einen Server sendet. In dieser Anfrage ist der Name oder die ID einer Methode, sowie die zugehörigen Parameter enthalten. Der empfangende Server führt die gewünschte Methode bei sich aus und liefert dem Client als Antwort den Rückgabewert der Methode.
Ein Vorteil von RPC ist, dass es sehr einfach und schnell möglich ist Methoden der Services für Clients und andere Services Verfügbar zu machen. Über die Kommunikation muss man sich nahezu keine Gedanken machen\cite[S.91]{buildingms}.

Bei Verwendung von RPC werden in der Implementierung Stubs und Skeletons verwendet. Die Stubs sind dabei client-seitige Schnittstellen, die es ermöglichen Prozeduren die auf einer anderen Maschine zur Verfügung stehen aufzurufen wie eine lokale Prozedur. Der Stub sorgt dann für das marshalling (umwandeln) der Daten und den Transfer über das Netzwerk. Auf der Server-Seite empfängt das Skeleton, welches die gleichen Methodensignaturen wie der Stub bereithält, die Anfrage, sorgt für die Ausführung und liefert die Ergebnisse, ebenfalls über das Netzwerk, zurück (siehe Abb.~\ref{fig:images/img_rpc-call_alternative.eps}).\cite{krzyzanowski}

\image[1]{images/img_rpc-call_alternative.eps}{Ablauf eines RPC}

Doch die Nachteile überwiegen schnell und deutlich. Je nach Implementierung führt die Verwendung von RPC zu einer starken Bindung an eine bestimmte Sprache (bekanntestes Beispiel Java RMI). Auch verstecken RPC-Implementierungen die Komplexität der entfernten Aufrufe. Dies kann zu starken Performance-Problemen führen, wenn Entwickler an Interfaces arbeiten, von denen sie denken es seien lokale Methoden\cite[S.93]{buildingms}.

Die Verwendung von RPC macht die Weiterentwicklung von Systemen nicht einfacher. Ein Beispiel anhand einer Schnittstelle zur Instanziierung eines Kunden: Zusätzlich zur Erstellung eines Kunden anhand seines Namen und einer E-Mail Adresse soll es nun eine Möglichkeit geben diesen nur mithilfe seiner Mail-Adresse zu erstellen. Das reine hinzufügen einer neuen Methode in einem Interface löst das Problem in diesem Fall nicht. Im schlimmsten Fall benötigen alle Clients die diesen Service ansprechen die neuen Stubs und müssen allesamt neu Bereitgestellt werden\cite[S.94]{buildingms}. Und das bereits bei einer so kleinen Änderung.

\paragraph{\acf{REST}}\label{ch:rest} ist eine der meistgenutzten Programmierparadigmen für \acp{API} \cite{duvander}. Entworfen wurde \ac{REST} von Roy Fielding, der die Idee und Spezifikationen in seiner Dissertation \enquote{Architectural Styles and the Design of Network-based Software Architectures} veröffentlichte.\cite{fielding}. Insgesamt besteht Fielding auf die folgenden 6 Eigenschaften, die erfüllt sein müssen um eine REST-Konforme Anwendung zu schreiben (ein Beispiel dazu in Abb.~\ref{fig:images/img_rest-call.eps}):

\begin{itemize}
	\item Client-Server
	\item Stateless
	\item Cache
	\item Uniform Interface
	\item Layered System
	\item Code-On-Demand (optional)
\end{itemize}

Die erste dieser Eigenschaften (\enquote{Client-Server}) verlangt schlichtweg, dass eine Client-Server Architektur vorliegt, bei der ein Server Funktionalität bereitstellt, die der Client nutzen kann.
\enquote{Stateless} ist die zweite Eigenschaft, die von Fielding gefordert wird. Sie besagt, dass alle Informationen, die zum Verständnis einer Nachricht nötig sind auch mitgeliefert werden müssen, da weder der Client, noch der Server, Zustandsinformationen zwischen Anfragen speichern sollen.

Eigenschaft Nummer 3, \enquote{Caching}, fordert die Verwendung von Caching um Netzwerklast zu minimieren.

Das \enquote{Uniform Interface} ist eine der größten Unterschiede von REST zu anderen Netzwerkarchitekturstilen. Diese Eigenschaft ist erneut aufgeteilt in vier Unterpunkte:

\begin{description}  
	\item [Identification of resources] Jede, über eine URI erreichbare Information ist eine Ressource
	\item [Manipulation of resources through representations] Änderungen an Ressourcen erfolgen nur über Repräsentationen von Ressourcen, also zum Beispiel durch Formate wie XML oder JSON. Genauso gut kann eine Ressource aber auch in unterschiedlichen Darstellungsformen vom Server ausgeliefert werden.
	\item [Self-descriptive messages] Die für die Anwendung versendeten Nachrichten sollen selbstbeschreibend sein. Zur Erfüllung dieser Eigenschaft ist es unter anderem nötigt, Standardmethoden zur Manipulierung von Ressourcen zu verwenden.
	\item [\acf{HATEOAS}] ist ein wichtiger Teil der REST-Spezifizierung und beschreibt ein Konzept, nach dem Informationen Hyperlinks zu anderen Informationen enthalten. 
\end{description}

\image{images/img_rest-call.eps}{Beispiel einer REST-Konformen Anwendung \cite{restexample}}

Die fünfte Eigenschaft verlangt nach \enquote{Layered Systems}, also mehrschichtigen Systemen. Die Idee dahinter ist, dass die Komponenten nur die Komponenten im System kennen mit denen sie in direkter Interaktion stehen. Alles weitere bleibt verborgen.

\enquote{Code-On-Demand} ist die letzte und eine optionale Eigenschaft der Spezifikation. Fielding beschreibt hiermit die Erweiterung von Client-Funktionalität durch den Server. Dieser sendet, wie zum Beispiel mit Javascript der Fall, Code für den Client direkt an den Client.\cite{fielding}

REST wird, auch wenn die Spezifikation es nicht vorschreibt, in den häufigsten Fällen über HTTP-Protokolle genutzt \cite[S.97]{buildingms}. Dies rührt daher, dass beispielsweise die bekannten HTTP-Methoden POST, GET, PUT usw. es sehr einfach machen die geforderte homogene Verhaltensweise von Methoden auf allen Ressourcen umzusetzen. Auch wird HTTP gerne genutzt, da es bereits eine breite Masse an bestehenden Tools gibt die zur weiteren Qualitätsverbesserung eines Systems genutzt werden können, wie zum Beispiel Proxies und Load Balancer. Doch dies sind alles zunächst nur Vorteile von HTTP.

Die Verwendung von REST bietet viele Möglichkeiten die lose Kopplung zwischen Services zu ermöglichen. Dazu zählt unter anderem \ac{HATEOAS}. Um bei dem Kundenbeispiel aus \ref{rpcpara} zu bleiben: Wird die Information eines Kunden abgerufen, kann das Kundenobjekt zusätzlich zu den Kundendaten ein Feld enthalten welches zur Bestellliste dieses Kunden zeigt.

\lstinputlisting{code/customer.json}

Mit der Verwendung von \ac{HATEOAS} reicht es, wenn alle Clients die Kundendaten und deren Bestellungen verarbeiten, wissen, dass Kunden einen Link-Feld mit dem Typ \textit{orders} besitzen. Wenn also später Services unter anderen \acsp{URI} erreichbar sind, oder sich interne Datenstrukturen ändern müssen diese Clients nicht neu angepasst werden.

Ein weiterer Anwendungsfall für \ac{HATEOAS} ist die Nutzung der weiterführenden Links zur Bildung eines Zustandsautomaten (siehe Abb.~\ref{fig:images/img_hateoas-state-machine.eps}). Die mitgelieferten Informationen zu weiteren \textit{Ressourcen} helfen dabei dem Client einer Anwendung Manipulationsmöglichkeiten des empfangenen Objekts zu prüfen.

\image{images/img_hateoas-state-machine.eps}{Beispiel einer State-Machine mit HATEOAS \cite{hatsm}}

Somit gibt die \ac{API} bereits den späteren Prozess Endanwendung vor. Ein Nutzer könnte alleine durch dem einfachen folgen der mitgelieferten Links einen Ablauf entsprechend der Geschäftslogik abarbeiten.


\paragraph{Message Channel}

Neben den oben genannten Möglichkeiten der synchronen Kommunikation gibt es auch in der Welt der Microservices asynchrone Ansätze. Einer davon ist die der Messages. Anstelle des typischen Frage-Antwort-Ablaufs einer gängigen Client-Server-Kommunikation wird bei der Verwendung von Nachrichten auf Buffer gesetzt. Ein Service, der einen Auftrag an einen anderen Service senden möchte, schickt eine Nachricht an einen Message-Buffer. Der angesprochene Service bearbeitet die Nachrichten dann nach eigenem Gusto(siehe Abb.~\ref{fig:images/img_message-driven.eps}). Zu den Vorteilen dieser Art der Kommunikation zählt unter anderem, dass die Skalierung stark vereinfacht wird, da einfach weitere Konsumenten gestartet werden können, die Nachrichten aus dem Buffer abarbeiten\cite{messagingwolff}:

\image[]{images/img_message-driven.eps}{Message Channel}

\paragraph{Event-Driven Communication}

ist die zweite Art der asynchronen Kommunikation. Der größte Unterschied zum Ansatz der Messages liegt darin, dass bei der Verwendung von Events kein expliziter Empfänger genannt wird, sondern alle Services ein Event empfangen, und eigenständig entscheiden, ob sie dieses Event verarbeiten wollen, oder nicht(siehe Abb.~\ref{fig:images/img_event-driven.eps}).
Eines der größten Probleme dieses Ansatzes ist, dass der Empfang eines Events nicht sichergestellt werden kann, dafür ist es aber um einiges Einfacher use-cases Abzudecken, in denen Änderungen an einem Service an viele weitere propagiert werden müssen.\cite{messagingwolff}

\image[0.4]{images/img_event-driven.eps}{Event-Driven Communication}

\subsubsection{Datenformat - JSON oder XML?} 

Wenn die Entscheidung über die Art und Weise der Datenübertragung gefallen ist muss das Datenformat festgelegt werden. JSON und XML sind dabei die bekanntesten Formate. In den vergangenen Jahren ist dabei die Verwendung von XML im Gegensatz zu JSON in \acp{API} zurückgegangen\cite{duvander2}. Jedoch bieten beide Vor- wie Nachteile. JSON ist das einfachere Format und auch leichtgewichtiger, während XML einige sinnvolle Features wie z.B. link control (insbesondere für \ac{HATEOAS} interessant) bietet.\cite[S.101]{buildingms}.

\subsubsection{Autorisierung mit OAuth2}

Beim Thema der Clientautorisierung von Web-\acp{API} ist OAuth2 weit verbreitet, insbesondere auf Social Media Plattformen. Ausschlaggebender Faktor dafür ist die Problematik die sich bei verteilten Systemen mit Rechtesystemen gerne aufdrängt: Woher weiß ein Service wer der anfragende Nutzer ist und was er darf? Dieses Problem soll möglichst so gelöst werden, dass es gut in die Philosophie der Microservices integriert werden kann. Eine eigene Autorisierung je existentem Service ist somit keine zufriedenstellende Lösung. OAuth2 bietet an dieser Stelle eine passende Lösung. Mit OAuth2 ist es möglich die Informationen zu den Rechten eines Nutzers einfach über Services hinweg zu propagieren und die Forderung nach Zustandslosigkeit der Anfragen von REST (siehe Kapitel~\ref{ch:rest}) nicht zu verletzen.

Wichtig ist hierbei die Unterscheidung zwischen Autorisierung und Authentifizierung. OAuth2 bietet keine Möglichkeit der Authentifizierung, also beispielsweise der Frage nach einem Nutzernamen und Passwort, sondern kümmert sich Ausschließlich darum festzustellen, ob ein Nutzer die nötigen Rechte zum durchführen einer bestimmten Operation besitzt.\cite{degges}

\begin{sloppypar}
Allerdings ist es für die Verwendung von OAuth2 innerhalb eines Unternehmenskontext sinnvoll zusammen mit der Autorisierung auch eine Authentifizierungsmöglichkeit am Authentifizierungs-Service anzubieten. Dieser Service kann dann die bestehende Infrastruktur des Unternehmens zur Authentifizierung von Nutzern verwenden. Zusätzlicher Vorteil dieses Vorgehens ist, dass jeder Service der zusätzliche Informationen eines Nutzers erfragen möchte, dies an einer zentralen Stelle mithilfe des vom Nutzer propagierten Tokens einfach tun kann.
\end{sloppypar}

Ein Nutzer kann dann an einem Endpunkt des Authentifizierungsservice den OAuth2-Token erhalten, mit dem er sich an anderen Services autorisieren kann. Diese Services können einen weiteren Endpunkt nutzen, um etwa Nutzerinformationen anhand des Tokens abzurufen, die für eventuelle Identitätsanforderungen benötigt werden (siehe Abb.~\ref{fig:images/img_oauth2-user-service.eps}).

\image[1]{images/img_oauth2-user-service.eps}{Stark vereinfachte Darstellung der Verwendung eines Service, der sowohl Authentifizierung als auch Autorisierung anbietet.}

\subsubsection{Deployment}

Das Deployment ist in Microservice-Architekturen ein wichtiger Punkt, da eine gute Deployment-Infrastruktur auch für den Nutzen einer solchen Architektur entscheidend sein kann: keine downtime, kurze Zeit bis Lösungen den Kunden erreichen und einfa Es gibt verschiedene Ansätze, wie man eine Anwendung zum Einsatz bringen kann\cite{richardsondeploy}.

\begin{description}
	\item[Multiple Service Instances per Host] Bei diesem Ansatz werden ein, oder mehrere physikalische oder virtuelle Hosts genutzt, auf denen mehrere Instanzen von Services laufen.
	
	\item[Service Instance per \ac{VM}] Jeder zu deployende Service wird als \ac{VM}-Abbild gebündelt. Dieses Abbild wird dann zum starten eines Services genutzt. Vorteil davon ist, dass jeder Service komplett isoliert von den anderen Services laufen kann. Ein Nachteil ist der hohe Overhead, der durch die Nutzung einer \ac{VM} anfällt.
	
	\item[Service Instance per Container] Die sogenannten Container sind ein Virtualisierungsansatz auf Betriebssystem-Ebene. Im Gegensatz zu regulären \acp{VM} sind Container aber weitaus leichtgewichtiger und erzeugen einen weitaus geringeren Overhead.
\end{description}

\subsection{Monolithische Architektur}\label{ch:mon-arch}

Eine Monolithische Anwendung vereint die Gesamtlogik in einer einzigen Laufzeitumgebung und ist dadurch meist sehr groß. Die Aufteilung der Logik innerhalb der Anwendung findet nur auf Entwickler-Ebene statt. Dies wird durch verschiedene Module erreicht die nur über definierte Schichten hinweg miteinander kommunizieren. In der Entwicklung werden diese Module auch gerne physikalisch durch Auslagerung in Bibliotheken getrennt, um ein Abweichen von den vorgeschriebenen Kommunikationswegen zu unterbinden.

Entwicklungstools und IDEs unterstützen den Entwickler beim programmieren von monolithischen Systemen - historisch bedingt - sehr gut.
Das Deployment ist sehr einfach. Beispielsweise reicht es bereits, ein einfaches WAR-File auf einem Application-Server auszuliefern.
Auch die Skalierung ist einfach möglich, durch die Verwendung eines Load-Balancers und das starten von mehreren Instanzen der Anwendung.\cite{richardson}

\subsection{Vorteile von Microservices gegenüber Monolithen}\label{ch:ms-mon-pro}

Als Vorteile der Microservice-Architektur lassen sich nun folgende Punkte, die unter anderem auch von Newman in seinem Buch dargestellt werden, festhalten\cite{buildingms}:

\begin{description}
	
	%--------------
	\item[Klare Team-Organisation] Eine Gruppe von Entwicklern ist für die Entwicklung und Pflege eines jeden Service verantwortlich, anstatt ein großes Team zu einzelnen Teilkomponenten einer monolithischen Anwendung zuzuweisen.
	
	%--------------
	\item[Heterogentität] welche es erlaubt, für verschiedene Einsatzzwecke verschiedene Sprachen, Technologien sowie Sprach- und Technologieversionen zu verwenden, ohne das ganze System damit implementieren zu müssen. Dieser Punkt ist insbesondere Interessant, wenn Firmen zusammengeführt werden, deren Anwendungen auf verschiedenen Sprachen basieren.
	
	%--------------
	\item[Austauschbarkeit] Die kleinen abgegrenzten Systeme lassen sich mit viel weniger Aufwand gegen neuere oder bessere Implementierungen austauschen, ohne andere Komponenten zu Gefährden. Während dies bei Monolithischen Anwendungen zu unvorhersehbaren Problemen führen kann, weshalb in solchen Architekturen auch häufig kaum Änderungen durchgeführt werden. Dazu zählt auch die Eigenschaft, dass die Aktualisierung von Frameworks wesentlich einfacher von statten gehen kann. Während dies bei einem monolithisches System fast ein eigenständiges Projekt mit mehreren Tagen Arbeit verursacht, ist es möglich solche Updates bei Microservice-Architekturen Schrittweise in kleineren Aufgabenpaketen zu erledigen, die auch zwischen größeren Aufgaben bearbeitet werden können.
	
	%--------------
	\item[Schnelleres Deployment] Insbesondere kleine Änderungen führen bei monolithischen Systemen zu einem großen Overhead, während man in einer Microservice Architektur nur den Service neu deployen muss, der auch die implementierte Änderung enthält. Ein weiterer Vorteil vom schnelleren Deployment ist die Möglichkeit, durch Continous Delivery die verlangte code coverage von Tests geringer zu halten. Dies ist möglich, da Fehler im Produktivsystem schnell behoben werden können, und keine Tagelangen Ausfälle durch einen komplizierten und Langwierigen Bereitstellungsprozess entstehen.
	
	%--------------
	\item[Erhöhte Widerstandsfähigkeit gegenüber Fehlern im System] Die harten Grenzen von Microservices können eine Kaskadierung von Fehlern verhindern und somit entsteht bei korrekter angewendeter Architektur kein Gesamtausfall des Systems. Diese wird allerdings auch benötigt, wie in \ref{high-tolerance} nachzulesen ist. 
	
	%--------------
	\item[Bessere Skalierung] Auch wenn die Skalierung von Monolithen in der Theorie sehr einfach funktionieren sollte, tut sie das in der Praxis meist nicht. Das Problem: Monolithen lassen sich nur eindimensional Skalieren. Auf eine höhere Anzahl von Anfragen kann also durch das starten einer weiteren Instanz reagiert werden. Doch wenn Teilkomponenten von Überlast betroffen sind wird es ineffizient. Beispielsweise gibt es eine Teilkomponente die sehr CPU-Intensive Berechnungen durchführt. Es müssen weitere Instanzen gestartet und dadurch Ressourcen belastet werden, die von der CPU-Intensiven Komponente besser verwendet werden könnten.
	
	\image{images/img_mon-scaling-problem.eps}{Auslastungsproblem bei der Skalierung von Monolithen}
	
	Microservices lassen sich dagegen effizienter skalieren. Während monolithische Architekturen immer im ganzen skaliert werden, kann man in einer Microservice-Architektur genau die Services skalieren, die in diesem Moment mehr Leistung benötigen(vgl. Abb.~\ref{fig:images/img_mon-scaling-problem.eps} und Abb.~\ref{fig:images/img_ms-scaling.eps}).
	
	\image[]{images/img_ms-scaling.eps}{Scaling der gleichen Komponenten in einer Microservice-Architektur}
	
	\item[Zusätzliche Schwierigkeiten von Monolithen] Weitere Probleme nehmen bei Monolithen ab einem gewissen Entwicklungsstand überhand. Viele Leute arbeiten an einer Code-Basis die stetig wächst. Wenn über den Projektzeitraum neue Entwickler am Projekt teilnehmen, können diese zunächst leicht überfordert sein, da die anfängliche Modularität dadurch, dass sie nicht hart vorgeschrieben ist, stetig abnimmt. Dies führt auch dazu, dass bei Änderungen an großen Systemen nicht mehr direkt klar ist, welche Auswirkungen es auf andere Teilkomponenten und somit das Gesamtsystem gibt. Die Entwicklungsgeschwindigkeit verringert sich.
	
	Auch die IDE trägt dazu bei. Je größer, das Projekt, umso höher die Belastung für den Entwicklungsrechner. IDEs indizieren Source-Files um die Navigation zu vereinfachen. Je nach Projektgröße kann so der Arbeitsspeicher schnell ausgelastet sein.
	
\end{description}

Dies sind einige der Gründe, warum über Microservices so viel diskutiert wird und weshalb sich diese Arbeit insbesondere auf die Testerstellung von Anwendungen in dieser Architektur konzentriert. Besonders die genannten Vorteile der Skalierung, die Austauschbarkeit von verschiedenen Sprach- und Technologieversionen und die erhöhte Fehlertoleranz sind Hauptbeweggründe, warum auch it@M nun mehr auf die Microservice Architektur setzen möchte. Sie behandelt einen Großteil der Probleme, die bei bestehenden Systemen der Landeshauptstadt in der Vergangenheit aufgetreten sind.

\subsection{Nachteile von Microservices gegenüber Monolithen}\label{ch:ms-mon-cons}

Doch gibt es, wie bei jeder Architektur, nicht nur Vorteile.

\begin{description}
	\item[Verteilung] Verteilte Systeme sind durch die Art der Kommunikation, also über das Netzwerk, einer weiteren Fehlerquelle ausgesetzt und vor allem eins: langsamer als Kommunikation innerhalb eines geschlossenen Systems\cite{tradeoffs}. 
	
	\item[Hohe Fehlertoleranz benötigt\label{high-tolerance}] Die Komplexität der laufenden Services auf der Infrastruktur ist ein weiterer negativer Aspekt, den man jedoch auch positiv auffassen kann. Es wird durch die Modularität und lose Kopplung der Komponenten eine hohe Fehlertoleranz von den einzelnen Teilen des Systems abverlangt. Es muss mit viel mehr Fehlerquellen umgegangen werden, als es bei einem Monolithen der Fall ist. So zum Beispiel der Ausfall oder die nicht-Erreichbarkeit von anderen Services. Dies führt allerdings auch dazu, dass viel Zeit in die Ausfallsicherheit der einzelnen Komponenten gesteckt werden muss, was eine höhere Qualität zur Folge hat. Netflix nutzt, um diese Ausfallsicherheit sicherzustellen und zu testen, ein eigens Entwickeltes Tool namens \enquote{Chaos Monkey}\footnote{\url{https://github.com/Netflix/SimianArmy}}. Dieses Werkzeug sorgt für zufällige Ausfälle von Komponenten in der produktiven Infrastruktur.
	
	\item[Eventual Consistency] Dabei handelt es sich um ein weiteres Problem von verteilten Systemen. Daten über mehrere Instanzen eines Services Konsistent zu halten ist nahezu unmöglich. Es gibt keine hundert prozentige Sicherheit, dass alle Daten einer Anwendung überall aktuell verfügbar sind.
\end{description}

\subsection{Resümee des Vergleichs}

An dieser Stelle sollte noch erwähnt werden, dass die Idee hinter Microservices nichts neues ist. Bereits auf der Schicht des Betriebssystems wird dies deutlich. Auf UNIX-Systemen laufen lauter keine Prozesse und Dienste, die Unabhängig voneinander existieren und entwickelt wurden, und in ihrer Gesamtheit das Betriebssystem bilden.\cite{hoff}

Auch sind Monolithische Architekturen nicht gleich schlechter oder unnutzbar. Bestes Beispiel einer aktuellen \textit{Start-Up} Firma ist etsy\footnote{\url{https://www.etsy.com/about/?ref=ftr}}, die ihren Online-Flohmarkt in Form einer Monolithischen Anwendung entwickeln und erfolgreich betreiben. Nicht zu vergessen sind auch etablierte Softwaresysteme, die \textit{trotz} der monolithischen Architektur sehr erfolgreich sind. Dazu zählt als bekanntestes Beispiel SAP\footnote{\url{http://www.sap.com/germany/index.html}}.

Ebenfalls zu ergänzen ist, dass, auch wenn Microservices zunächst klein und kompakt wirken, auch diese eine hohe Komplexität beinhalten können. Nur ist diese Komplexität versteckt vor den Entwicklern die nicht daran arbeiten und es handelt sich meist um fachliche Komplexität, während Monolithen meist eine strukturelle Komplexität vorliegt. Das aufteilen von Logik und Kernkompetenzen ist auch auf reiner Code-Basis möglich und wird auch so umgesetzt, solange sich an Interface-Definitionen und eine Strenge Trennung von verschiedenen Komponenten eines Monolithen gehalten wird.\cite{hoff}

\section{Vorgegebene Architektur der \acf{LHM}}\label{ch:arch-itm}

\subsection{Backing Services der Architektur}

Von it@M sind bereits Entscheidungen zum Aussehen einer Anwendung in Microservice-Architektur getroffen worden. Als Infrastrukturelle Unterstützung gibt es innerhalb der Anwendung einen Authentifizierungs-Service, der sowohl eine Authentifizierung über die städtische Infrastruktur bietet, als auch über eine eigene Nutzerverwaltung (siehe Abb.~\ref{fig:images/img_arch-stadt-gesamt.eps}).

Des weiteren existiert ein Discovery-Service, der im System wie eine Art \acs{DNS} agiert\footnote{Wird für die Orchestrierung der Services in Produktion Kubernetes (\url{https://kubernetes.io/}) verwendet, wird tatsächlich ein DNS-Service zur Service-Discovery genutzt. Jede Instanz eines Service erhält einen DNS-Namen und die Adresse ist darüber auflösbar.(siehe dazu \url{https://kubernetes.io/docs/admin/dns/})}. Dieser Service erlaubt es allen im System beteiligten Services eine lokales Abbild seines Registers aller verfügbaren Services abzurufen, und über dieses Register Services anzusprechen. Im Gegensatz zu einem normalen \ac{DNS} muss der Discovery-Service allerdings nicht manuell mit Daten gespeist werden, sondern verlangt es, dass Services sich eigenständig dort registrieren. Sobald also ein neuer Service innerhalb des Systems startet, meldet dieser sich mit seinem Namen und Netzwerkadresse an und ist ab sofort für die anderen Services verfügbar.

Zusätzlich existiert ein Configuration Service der die zentrale Steuerung von gemeinsamen Einstellungen erlaubt. Die größte Varianz bei Vergleich von Enwicklungs-, Test- und Produktionsumgebungen existiert in den Konfigurationen der Services. Dazu zählen u.a. Anbindungen an Datenbanken oder auch die IP-Adressen der zuvor genannten Backing Services\cite{wiggins}. Ein Configuration Service lagert diese Konfigurationen aus dem Source Code der Services selber aus, um Konfigurationsänderungen an diesen Parametern auch zur Laufzeit des Systems ohne ein neues Deployment zu ermöglichen.

Des weiteren gibt es einen Edge Server. Dieser fungiert als eine Art Gateway. Er bietet eine Schnittstelle für die Gesamtanwendung und verhindert somit, dass die Komplexität der vielen kleinen Services für weitere Anwendungen und Nutzer nach außen getragen wird. Dies ist auch insbesondere dann wichtig, wenn das Frontend über Javascript mit den Services des Systems kommunizieren soll, da somit keine \ac{CORS}-Anfragen mehr benötigt werden, weil auch das Frontend über diesen Edge Server bereitgestellt werden kann.

Neben zur Grundstruktur kann es in einer Anwendung mehrere Services geben, die alle innerhalb ihrer Kontextgrenzen Aufgaben erfüllen. 

\image{images/img_arch-stadt-gesamt.eps}{Architekturvorgabe it@M}

\subsection{Kommunikation zwischen den Komponenten}

Zum Start der Anwendung registrieren sich alle Komponenten beim Discovery Service um für Anfragen erreichbar zu sein.

Der Konsument der Anwendung, sei es ein Nutzer oder eine weitere Software, kommuniziert ausschließlich über den Edge Server mit den internen Komponenten. Zunächst besteht für den Konsumenten die Möglichkeit, sich beim Authentication Service zu Authentifizieren und mithilfe eines OAuth2 tokens für spätere Anfragen zu Autorisieren. Wird eine Anfrage gestellt wird über den Discovery Service der zur Bearbeitung der Anfrage benötigte Service gesucht und die Anfrage an diesen gestellt. Bei allen gesicherten Endpunkten innerhalb der Anwendung findet immer eine Absprache mit dem Authentifizierungsservice statt, um auf eine Änderung der Rechte eines Nutzers sofort reagieren zu können. Bei ungesicherten Ressourcen ist diese Abstimmung natürlich nicht nötig (siehe Abb.~\ref{fig:images/img_arch-stadt-gesamt-full.eps}).

Der Configuration Service aktualisiert bei Bedarf die Konfigurationen aller beteiligten Services innerhalb der Anwendung.

\image{images/img_arch-stadt-gesamt-full.eps}{Kommunikation innerhalb der Architektur}

\subsection{Interne Struktur der Services}

Jeder Service der Anwendung hat intern eine gleiche Aufteilung. In Abbildung~\ref{fig:images/img_architecture_testing.eps} ist eine graphische Darstellung zu finden. Ziel ist es, für eine Standardisierung innerhalb der Services zu sorgen, um Entwicklern den Einstieg zu erleichtern, egal an welchem Projekt aktuell Microservices entwickelt werden. Nach außen werden, REST-konform, nur die Ressourcen über \acp{URI} angeboten. Diese Ressourcen bestehen entweder aus Entitäten oder Geschäftsanwendungen. Entitäten werden über sogenannte Repositories als Ressource bereitgestellt und über eine \ac{ORM}-Bibliothek in einer Datenbank persistiert. Der Begriff des Repositories stammt ebenfalls aus der Welt des \ac{DDD} und lässt sich anhand einer Definition von Edward Hieatt and Rob Mee gut erläutern\cite{hieatt}:

\myquote{Mediates between the domain and data mapping layers using a collection-like interface for accessing domain objects.}

Ein Repository stellt also eine Verhandlungsebene zwischen den Schichten der Domäne und der Datenpersistierung.
Zum zusätzlichen Eingriff durch Entwickler können, kann auf verschiedene Events mithilfe von Listenern reagiert werden. Zur Kommunikation mit anderen Services existiert ein \textit{Security REST Client}, der diese für den Entwickler, durch automatische Verwendung des Discovery und Authentication Service, so einfach wie möglich machen soll. 

\image{images/img_architecture_testing.eps}{Aufbau eines Service innerhalb der it@M-Architektur}

% ----------------------------------------------------------------------------------------------------------
% Software Testen
% ----------------------------------------------------------------------------------------------------------

\section{Software Testen}

Das Testen von Software ist der Prozess, ein Programm auszuführen mit dem Ziel Fehler zu finden und zu beheben. Im Gegensatz zu physikalischen Systemen kann Software auf unterschiedliche und unvorhersehbare Weisen defekte aufweisen. Dies liegt unter anderem daran, dass die vielen Fehler in Anwendungen nicht durch Fehler in Produktionsabläufen, sondern durch falsche oder schlechte Design-Entscheidungen entstehen.\cite{pantesting}

\subsection{Bekannte Testmethoden}\label{ch:soft-test}

Zunächst kann das Testen von Software in Funktionale und nicht-funktionale Tests getrennt werden (siehe Abb.~\ref{fig:images/img_testmethods.eps}). Zur weiteren Erklärung müssen zunächst die Begriffe der funktionalen und nicht-funktionalen Anforderungen beschrieben werden. Einfach gesagt beschreiben funktionale Anforderungen \textit{was} das System tun sollte (Kundenanforderungen) und nicht-funktionale Anforderungen \textit{wie} das System funktioniert(Technische Anforderungen).\cite{erikssonreq}

\image{images/img_testmethods.eps}{Funktionale und nicht-funktionale Testmethoden}

Funktionale Tests beschäftigen sich nur mit den funktionalen Anforderungen einer Anwendung und stellen fest, ob, und wie gut diese Anforderungen erfüllt werden.\cite{erikssontesting}. Funktionales Testen wird häufig in vier Komponenten aufgeteilt, die auch meist in Reihenfolge abgearbeitet werden (siehe Abb.~\ref{fig:images/img_testmethods-functional.pdf}).\cite{inflectra}

\begin{description}
	\item[Unit Tests] testen einzelne Module einer Software und werden meist direkt vom Entwickler dieses Teilmoduls implementiert. Sie bewegen sich meist auf Klassenebene um die Komponenten so klein  wie möglich zu halten. In testgeleiteten Programmierparadigmen werden diese Tests bereits vor der Implementierung der eigentlichen Komponente umgesetzt.\cite{inflectra}
	\item[Integration Tests] sind der nächste Schritt, wenn mehrere Komponenten durch Unit Tests abgedeckt sind. Diese getesteten Teile der Anwendung werden dann zusammengeführt und ihre Zusammenarbeit wird geprüft.\cite{inflectra}
	\item[System Tests] testen abschließend das Gesamtsystem. Also das Zusammenspiel aller Unit- und Integrationsgetesteten Teilkomponenten.\cite{inflectra}
	\item[Acceptance Tests] sollen sicherstellen, dass alle Produkt und Projekt-Anforderungen zur Zufriedenheit des Kunden erfüllt wurden. Sie stellt die finale Phase der funktionalen Tests dar.\cite{inflectra}
\end{description}

\image{images/img_testmethods-functional.pdf}{Umfang der verschiedenen Testmethoden}

Nicht-funktionales Testen evaluiert die \textit{Bereitschaft} des Systems.\cite{erikssontesting} Zu den nicht-funktionalen Testmethoden zählen unter anderem die folgenden\cite{inflectra}:

\begin{description}
	\item[Performance-Tests] Mit diesen wird festgestellt, wie gut eine Anwendung mit vielen Anfragen umgehen kann.
	\item[Security Testing] konzentriert sich auf das prüfen der Anwendung in Hinblick auf Vertraulichkeit von Informationen, Integrität, Erreichbarkeit und die Authentifizierung.
	\item[Usability Tests] sind eine eher objektive Testmethode mit deren Hilfe die Benutzerfreundlichkeit einer Anwendung sichergestellt werden soll. Es wird besonders auf die Erlernbarkeit der Anwendung, Effizienz bei der Nutzung, Zufriedenstellung des Nutzers und Einprägsamkeit Wert gelegt.
	\item[Compatibility Tests] Tests, die Prüfen, ob die Anwendung auf allen geforderten Betriebssystemen, Browsern und/oder Hardware-Plattformen lauffähig ist und alle Funktionen sich kongruent verhalten.
\end{description} 

\section{Konzeption: Testen von Microservices und generative Testerstellung}\label{ch:ms-test}
\label{sec:testingms}

Durch einige spezielle Eigenschaften von Microservices ergeben sich teilweise auch spezielle Testanforderungen die in der Entwicklung von Tests Beachtung finden müssen. Allerdings wird sich auch bei Microservices auf altbekannte Methodiken gestützt, insbesondere in den Grundlegenden Bereichen der Unit- und Integrationstests.

\subsection{Unit Testing}

Auch bei Microservices beginnt das Testen eines Gesamtsystems auf der niedrigsten Schicht. Auch hier gilt es, dass Unit Tests generell auf Klassen- oder Methoden-Level geschrieben werden.\cite{clemson} Im Falle des Architekturmodells von it@M beschränken sich Unit Tests auf das Testen der Geschäftslogik, sowie der verschiedenen Listenern (siehe Abb.~\ref{fig:images/img_unit-testing.eps}). Die Repositories und Entitäten sind durch das verwendete Framework nicht mit reinen Unit Tests testbar, sondern werden erst in den folgenden Integrationstests getestet.

\image{images/img_unit-testing.eps}{Unit Testing Scope \cite{clemson}}

\subsection{Integration Testing}

An dieser Stelle ist die Kernfunktionalitäten der Services, also die Stellen, an denen nicht reiner Code zur Persistierung und Bereitstellung von Schnittstellen, sichergestellt. Doch die korrekte Zusammenarbeit der getesteten Komponenten ist noch nicht gewährleistet. Bei den Integrationstests werden nun mehrere Module zusammengefasst um das Zusammenspiel dieser Komponenten zu testen.\cite{clemson} Auch hier wird also nicht vom Regelfall aus Kapitel \ref{ch:soft-test} abgewichen. Auch wird in diesem Stadium das Zusammenspiel des Data Mappers und der angebundenen Datenbank, sowie die daraus resultierenden Ressourcen auf Korrektheit geprüft (siehe Abb.~\ref{fig:images/img_integration-testing.eps}). Wichtig an dieser Stelle ist es, aufgrund der fortschrittlichen Implementierung von modernen \acp{ORM}, sicherzustellen, dass die Testdaten nicht nur temporär vorgehalten wurden, sondern tatsächlich in die Datenbank geschrieben und von dort herausgelesen wurden, um etwaige Fehler auf diesen Schichten auszuschließen.\cite{clemson}

\image{images/img_integration-testing.eps}{Integration Testing Scope \cite{clemson}}

\subsection{Component Testing}

Unit- und Integrationstests sorgen dafür, dass an dieser Stelle bereits Sichergestellt ist, dass die Logik in den einzelnen Komponenten garantiert so funktioniert wie sie beabsichtigt ist. Doch ist dies der Punkt, an dem sich das Testen von Microservices vom Testen von monolithischen Anwendungen in Details unterscheidet. Da der nächste Schritt nach dem testen der einzelnen Teilkomponenten eines Service darin besteht, den gesamten Service zu testen, treten Probleme auf, sobald dieser zur korrekten Funktionalität externe Abhängigkeiten besitzt.

Component testing tritt an dieser Stelle ein. Es eignet sich besonders gut für Microservice Architekturen, da die sich diese leicht in die getrennt zu testenden Komponenten aufteilen lassen: Jeder Service entspricht einer Komponente. Externe Kommunikationspartner werden durch \textit{test doubles} ersetzt, dadurch wird eine Isolierung des Service für den Test sichergestellt (siehe Abb.~\ref{fig:images/img_component-testing.eps}).\cite{clemson} Ein \textit{test double} ist eine generische Umschreibung für ein Objekt welches man zu Testzwecken gegen ein produktive Komponente austauscht. Dazu zählen u.a. \textit{dummys}, die zwar weitergereicht, aber nie verwendet werden, oder auch \textit{stubs}, die für bestimmte vordefinierte Anfragen vordefinierte Antworten liefern.\cite{fowlertestdouble}

\image{images/img_component-testing.eps}{Component Testing Scope \cite{clemson}}

Bei Services die komplexere Integrations- oder Bootlogik besitzen kann es auch sinnvoll sein, gemockte Services durch externe stubs von den benötigten Services zu ersetzen. Dadurch wird die Testlogik zwar zu teilen in den Test-Harnisch, der für die korrekte Ausführung der Tests verantwortlich ist, geschoben, doch können zusätzliche Fehlerquellen durch die Verwendung von echten Netzwerkanfragen bereits jetzt erkannt werden. Zu beachten ist jedoch, dass sich dadurch auch die Zeit der Testausführung erhöhen kann.\cite{clemson} Ein Test-Harnisch ist die Zusammenstellung aller für die Ausführung von Tests nötigen Informationen. Dazu zählen auch Konfigurationen, die beispielsweise stubs und andere Komponenten für die Tests starten.

\subsection{\acf{CDCT}}

Durch die Kombination von unit-, integration- und component testing ist bereits eine hohe Abdeckung eines Microservice erreicht und es ist sichergestellt, dass der Service die Geschäftslogik korrekt implementiert hat. Doch für das Gesamtsystem reicht dies nicht aus. Der wahre Wert der Anwendung entsteht erst durch das Zusammenspiel der verschiedenen Services und in dem Gesamtbild, welches diese Formen. Momentan ist nicht durch Tests sichergestellt, dass externe Komponenten die eigenen Anforderungen so erfüllen wie diese gewünscht sind, oder dass alle Services der Domäne einwandfrei zusammenspielen.\cite{clemson} Um diese Lücken zu schließen bieten contract testing und end-to-end testing abhilfe.

Da End-to-End-Tests, also Tests bei denen die gesamte Domäne gestartet und getestet wird sind sehr aufwändig sind und nur sehr lange Feedback-Zyklen bieten, steht das contract testing als Schicht davor.\cite{clemson}

Contract testing basiert auf der Forderung, dass zwischen einem Konsumenten und einem Produzenten ein Vertrag besteht, den der Produzent einzuhalten hat. Dieser Vertrag besteht beispielsweise aus Erwartungen bezüglich In- und Outputs eines Interface. Jeder Konsument hält mit dem Produzenten einen Vertrag der für seinen Anwendungsfall einzuhalten ist. Wenn am Produzenten Änderungen vorgenommen werden, muss sichergestellt sein, dass alle Verträge ihre Gültigkeit behalten, sodass die abhängigen Konsumenten keine Funktionalität einbüßen müssen.\cite{clemson}

Bei Microservices besteht das im Vertrag festgehaltene Interface aus der öffentlichen \ac{API} dieses Service. Jeder Betreiber eines Konsumierenden Service schreibt eine Testsuite, die unabhängig voneinander nur die verwendeten Schnittstellen des Produzenten prüft. Im Idealfall werden diese Testsuites dann in der build pipeline des produzierenden Service eingebunden, sodass die Entwickler des Produzenten Auswirkungen auf die Konsumenten sofort erkennen können.\cite{clemson}

Diese Art von Test ist nicht gleichzusetzen mit component testing. Es wird nicht die tiefgehende interne Logik eines Service geprüft, sondern lediglich die Erwartungen von in- und outputs sowie die Einhaltung von Latenz und Durchsatz gegenüber der Realität validiert.\cite{clemson}

\image{images/img_contract-testing.eps}{Contract Testing \cite{clemson}}

Ein großer Vorteil von dieser Vorgehensweise ist der, dass der Produzierende Service viel leichter Änderungen durchführen kann, da er weiß, welche Schnittstellen und Felder von seinen Konsumenten wirklich benötigt werden. So könnte der Produzent aus Abbildung~\ref{fig:images/img_contract-testing.eps} eine Ressource anbieten die die Felder \textit{id}, \textit{name} und \textit{age} anbietet. Konsument A benötigt nur die \textit{id} und \textit{name}-Felder, somit wird in den contract tests zu Vertrag A auch nur die existenz dieser beiden Felder geprüft. Konsument B benötigt nur das \textit{id} und \textit{age}-Feld, während Konsument C alle Felder benötigt, um korrekt zu funktioniere.\cite{clemson}

Sollte nun ein weiterer Konsument zur Anwendung hinzukommen, der neben dem Vornamen auch einen Nachnamen benötigt, können die Betreiber des Produzenten sich dazu entscheiden, dass alte \textit{name}-Feld als veraltet zu markieren und ein neues Zusammengesetztes Objekt aus Vor- und Nachname anbieten. Durch die Durchführung aller component tests wird schnell deutlich, dass die Konsumenten A und C von dieser Änderung betroffen und somit informiert werden müssen. Sobald die beiden Konsumenten an die neue Schnittstelle angepasst sind, kann das alte Feld \textit{name} aus der Ressource entfernt werden. Laufen dann alle contract tests ohne Fehler, ist die Migration auf die neue Schnittstelle erfolgreich.\cite{clemson}

\subsection{End-To-End Testing}

Der letzte Schritt zur Sicherstellung, dass alle funktionalen Anforderungen des Systems erfüllt sind ist das end-to-end testing. Im Gegensatz zu den vorherigen Testmethoden soll durch diese Testmethode festgestellt werden, dass sämtliche Geschäftsanforderungen vom System gelöst werden, unabhängig von der Architektur der verwendeten Komponenten.~\cite{clemson}

Das Gesamtsystem wird für end-to-end tests als black box gesehen und es werden möglichst viele der Aufgaben des Systems über öffentliche Schnittstellen getestet. Da in Microservice-Architekturen aufgrund ihrer hohen Modularität viele Lücken zwischen den Komponenten entstehen, helfen diese Tests auch die Konfigurationen von etwa verwendeter Infrastruktur wie Firewalls, Proxis und Load-Balancern zu testen.\cite{clemson}

Verlangt ein System nach Nutzer-Interaktion kann die durch einen Service bereitgestellte GUI durch die Verwendung von etwa Selenium\footnote{\url{http://www.seleniumhq.org/}} getestet werden. Dazu werden typische Nutzerabläufe, beispielsweise aus user stories, aufgenommen und automatisiert ausgeführt, sodass ein Zusammenspiel des Gesamtsystems nötig ist.\cite{clemson}

Für Systeme die keine Nutzeroberfläche bieten werden direkt die öffentlichen \acp{API} der Services angesprochen um diese zu manipulieren. Dies geschieht durch die Verwendung eines, möglichst automatischen, HTTP-Clients.\cite{clemson}

\image{images/img_end-to-end-testing.eps}{End-To-End Testing Scope mit externen Abhängigkeiten \cite{clemson}}

Sollte nicht ein Team Komponentenverantwortlich für die Gesamtanwendung sein,  werden bei end-to-end tests auch externe Services mit eingesetzt (siehe Abb.~\ref{fig:images/img_end-to-end-testing.eps}). Es gibt allerdings Fälle in denen es sinnvoll sein kann diese aus dem scope zu exkludieren. Bei Abhängigkeiten die nicht einmal mehr in den Händen der Firma selbst, sondern bei Drittanbietern liegen gibt es viele Faktoren, wie etwa die Stabilität der Systeme des Drittanbieters selbst, die die Testergebnisse für das eigene System verfälschen können. In solchen Situationen ist es Sinnvoll, auch in end-to-end tests noch stubs dieser externen Systeme zu verwenden, um sinnvolle Resultate für die eigene Anwendung zu erhalten. Dadurch verliert zwar die Testmethode selbst ein wenig an Vertraulichkeit in Bezug auf die Resultate, jedoch bleiben diese Resultate wenigstens stabil.\cite{clemson}

End-to-end tests haben einen großen Nachteil: Sie sind die Tests mit den meisten Faktoren und Komponenten und haben dadurch auch die größte Spanne an möglichen Schwachstellen die zu einem Fehlschlagen der Tests führen können. Dies führt dazu, dass die Laufzeit dieser Tests extrem hoch sein kann und auch das pflegen der Test suites ist mit einem großen Arbeitsaufwand verbunden.\cite{clemson} Daher ist es wichtig beim schreiben eben dieser auf einige Punkte acht zu geben.

\begin{description}
	\item[So wenige end-to-end tests schreiben wie möglich] Da ein hoher Vertrauensgrad bereits durch die vorhergehenden Teststufen erreicht werden kann, und end-to-end tests nur noch das Zusammenspiel der bereits durchgängig getesteten Komponenten darstellen soll, sollte nicht zu viel Zeit in zu viele Testszenarien gesteckt werden. Am besten ist es, sich ein Zeitbudget zurechtzulegen, welches man bereits ist für die Laufzeit aller end-to-end tests abzuwarten. Sobald das Zeitbudget durch die Laufzeit überschritten wird, sollten die Tests, die am wenigsten Mehrwert bieten detektiert und gelöscht werden. Das Zeitbudge sollte jedoch in Minuten, nicht in Stunden festgelegt sein.\cite{clemson}	
	
	\item[Auf user stories und Anwendungsfälle fokusieren] Wenn man sich beim end-to-end testing auf reale Anwendungsfälle der Software beschränkt entsteht auch nicht das Problem, dass zu viele Tests dieses Typs geschrieben werden. Es werden nur die Hauptsituationen in denen sich ein Nutzer wiederfindet geprüft, alle anderen Szenarien werden in den meisten Fällen durch die anderen Testmethoden sichergestellt.\cite{clemson}
	
	\item[Sinnvolle definition des scopes] Wie bereits im vorherigen Abschnitt beschrieben, kann es sinnvoll sein externe Services aus dem end-to-end testscope auszuklammern. Dies kann auch für \acsp{GUI} von Vorteil sein, sollten diese Beispielsweise Ungenauigkeiten in den Testergebnissen verursachen. Wie bereits zuvor erwähnt tauscht man die Abdeckung der Tests gegen die Zuverlässigkeit der Ergebnisse.\cite{clemson}
	
	\item[Auf \ac{IaC} zur Wiederholbarkeit von Tests setzen] \ac{IaC} ist eine Herangehensweise die, insbesondere in Agilen Entwicklungsteams, für konsistentere Testumgebungen sorgen soll. Wenn Server manuell aufgesetzt und konfiguriert werden, kann dies zu Problemen führen. Es werden bestimmte Versionen von Softwarepaketen installiert, Konfigurationen am System werden vorgenommen etc. Desto mehr dieser von Hand getätigten Schritte führen zur Bildung von Servern, die keinem anderen mehr gleichen. Martin  Fowler bezeichnet diese als \textit{Snowflake Server}\cite{fowlersnow}. Es ist schwer Testumgebungen an die Produktionsumgebungen anzupassen, desto mehr solcher \textit{Schneeflocken} durch das Rechenzentrum treiben. Der Ansatz von \ac{IaC} geht dahin, diese Konfigurationen in Code zu definieren, sodass die Konfiguration der Test- und Produktionsumgebungen in einer Datei festgehalten ist und diese sogar mit der Anwendung selber in einem \acs{VCS} abgelegt werden kann. Auch wird es dadurch möglich, Infrastrukturen bei Änderungen automatisch zu starten und Änderungen an mehreren Systemen gleichzeitig durchführen zu lassen.\cite{fowlersnow}
	
	Beim end-to-end testing spielt dies eine große Rolle, da durch \ac{IaC} homogene Laufzeitumgebungen geschaffen werden können, sodass Fehler durch unterschiedlich konfigurierte Server und Systeme ausgeschlossen werden können.\cite{clemson}
	
	\item[Tests Datenunabhängig gestalten] Anstatt auf bereits existente Datensätze zu Vertrauen, sollten die end-to-end tests selbst die Möglichkeit zur Erzeugung von Daten über die öffentlichen Schnittstellen der Anwendung nutzen, um bei den Tests ständig mit den gleichen Daten zu arbeiten. Wird dies nicht so umgesetzt, kann es zu fehlerhaften Testergebnissen kommen, die jedoch nicht Fehler in der eigentlichen Anwendung als Ursache haben.\cite{clemson}
\end{description}

Doch kann man end-to-end testing auch weitaus weniger strukturiert gestalten. An dieser Stelle kommt monkey testing ins Spiel. Anstatt Geschäftsanforderungen in end-to-end Tests zu modellieren verfolgt dieser Ansatz die Idee, eine Anwendung durch lauter zufällige Anfragen zu testen, die keiner menschlichen Logik entsprechen. Dabei wird das System für längere Zeit den zufälligen Anfragen ausgesetzt und es werden auftretende Fehler dokumentiert. Der Vorteil dieses Vorgehens liegt natürlich darin, dass sich keine Gedanken über die Testfälle selber gemacht werden müssen, allerdings sollte man diese Art der end-to-end Tests bestenfalls in Kombination mit dem abbilden von Geschäftslogiken verwenden.

Durch das aufbrechen einer Anwendung in mehrere sauber getrennte Services treten zu teilen neue Grenzen auf, die zuvor wahrscheinlich unentdeckt geblieben werden. Diese neuen Grenzen erlauben es auch, Tests weitaus flexibler zu gestalten, als es bei Monolithen möglich wäre.

Hat ein Service eine immens wichtige Aufgabe im Sinne der Geschäftsanforderungen, ist es sinnvoll die volle Bandbreite an möglichen Testmethoden im höchsten Detailgrad auf diesen Service anzusetzen um sein korrektes Verhalten zu garantieren. Wird allerdings ein weiterer Service betrachtet, der weniger wichtige Aufgaben, oder gar experimentelle Funktionen bereitstellt, kann es Sinn machen, die Entwicklungszeit auf andere Gebiete zu fokussieren und nur ein paar der hier vorgestellten Herangehensweisen zu implementieren.\cite{clemson}

% ----------------------------------------------------------------------------------------------------------
% Implementierung
% ----------------------------------------------------------------------------------------------------------
\section{Implementierung}\label{ch:implementierung}

\subsection{Referenz-System}

\subsubsection{Architektur}

Das Referenzsystem basiert auf der in Kapitel~\ref{ch:arch-itm} angesprochenen Architektur. Es wird das Spring-Framework verwendet. Auslöser dafür ist, dass bei it@M größtenteils Java-Entwickler arbeiten und diese bereits mit Java EE viel Erfahrung gesammelt haben. Da das Spring-Framework auf viele Konzepte von Java EE aufbaut, erleichtert dies den Einstieg in die Microservice-Welt für kommende Entwicklungen.

Die Services kommunizieren über eine REST-\ac{API}, die ihre Daten im JSON-Format überträgt. \ac{HATEOAS} wird über Spring \ac{HATEOAS}\footnote{\url{http://projects.spring.io/spring-hateoas/}} genutzt.

Als Datenbank in der Entwicklungsumgebung kommt eine In-Memory-Datenbank von H2 zum Einsatz. In der Produktion größtenteils Oracles JDBC-Datenbank in der Version 7, aber auch MySQL wird verwendet.

Zur Authentifizierung wird ein Authentifizierungs-Service genutzt, der zusätzlich zur internen Nutzer- und Rechteverwaltung auch eine Anbindung an das Stadtweite \ac{LDAP} bietet.

Das Problem der Findung von Service wird mithilfe von Netflixs Eureka-Service gelöst. Dieser bietet innerhalb einer Domäne einen Zentralen Anlaufpunkt für alle Services um sich dort zu vermerken, sowie Informationen über die Adressen von anderen Services einzuholen.

Clients können mit der Domäne über ein \ac{API}-Gateway kommunizieren (Netflix Zuul). Dieses öffnet nach außen eine einzige Schnittstelle, über die sowohl eine graphische Nutzeroberfläche aufgerufen, als auch die einzelnen Services kontaktiert werden können. Auch ermöglicht das Gateway den Zugriff auf bestimmte Endpunkte zur sperren, was für die Landeshauptstadt insbesondere für Software interessant ist, die sowohl interne, als auch externe Schnittstellen bieten soll.

Auch werden Docker-Konfigurationen verwendet, um die Continous Delivery mithilfe eines Container-Frameworks zu ermöglichen.

\subsubsection{Beispiel-Anwendung}

Es wird ein vereinfachtes Modell eines Online-Shops, beispielhaft \textit{Kongo} genannt, als Anwendungszweck herangezogen. Das Model in der Barrakuda-Sprache sieht folgendermaßen aus:

\lstinputlisting{../ReferenceSystem/.mdsd/referencesystem.barrakuda}

Das Referenz-System besteht, wie in Abbildung~\ref{fig:images/img_referenz-system-arch.eps} zu sehen, aus 3 Services. Dem \textit{shoppingcart}-Service, dem \textit{ordering}-Service und dem \textit{warehouse}-Service. Im \textit{warehouse} werden alle verfügbaren Produkte des Online-Shops verwaltet. Wenn ein Kunde nun eines der Produkte bestellen möchte, wird eine Anfrage an den \textins{shoppingcart}-Service gesendet, die die OID des Produktes, sowie die gewünschte Anzahl enthält.

Möchte ein Kunde dann eine Bestellung aufgeben, geht eine Anfrage mit der OID des virtuellen Einkaufswagens an den \textit{ordering}-Service, der zusätzlich Schnittstellen zum erstellen einer Rechnung, sowie zum stornieren einer Bestellung bietet.

Der mit generierte Authentifizierungsservice dient als Kundenverwaltung.

\image[]{images/img_referenz-system-arch.eps}{Aufbau des Referenzsystems zur Testimplementierung}

Wichtig ist, dass im Referenz-System keinerlei Logik zu den modellierten Geschäftsanwendungen implementiert wird, sondern lediglich die im Abschnitt~\ref{sec:testingms} angesprochenen Testmethoden.

\subsection{Frameworks zum Umsetzen von Test-Strategien}\label{ch:ms-test-frw}

Java als Sprache und Spring als Framework schränken die Auswahl von sinnvollen Testframeworks bereits ein und auch die persönliche Erfahrung durch Verwendung eben dieser spielt eine wichtige Rolle. Für Unit-, Integration- und Component-Testing wird JUnit unter Zuhilfenahme des Spring Testframeworks \textit{spring-boot-test}\footnote{\url{https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-testing.html}} genutzt.
Auch für das \ac{CDCT} bietet Spring eine eigene Lösung namens \textit{Spring Cloud Contract}\footnote{\url{https://cloud.spring.io/spring-cloud-contract/}} die sich leicht und gut in die bestehende Infrastruktur mithilfe von Maven einbinden lässt.
Für end-to-end tests wird Apache JMeter\footnote{\url{http://jmeter.apache.org/}} genutzt. Die Wahl des richtigen Tools war in diesem Bereich eher schwierig, da es eine sehr große Anzahl an Möglichkeiten gab. JMeter wurde unter anderem daher gewählt, dass Apache-Produkte bereits einen hohen Verbreitungsgrad haben und eine Verwendung in Produktion somit sehr wahrscheinlich ist. Außerdem überzeugt die Kombination aus textueller Beschreibung und ausgesprochen intuitiver grafischer Bedienoberfläche zur Erstellung von Tests. Somit ist nicht nur eine einfache Generierbarkeit gewährleistet, sondern auch eine unkomplizierte Erweiterung dieser durch einen Entwickler.

\subsection{Generativer Ansatz - Was zu beachten ist}

Die im Referenzsystem implementierten Tests dienen als Basis für die spätere Verwendung in Barrakuda von it@M. Barrakuda verwendet sogenannte Templates, die anhand der gegebenen Informationen aus dem vom Nutzer erstellten Model befüllt werden. Daher muss bei der Implementierung der Testmethoden unbedingt Rücksicht auf die zur Verfügung stehenden Informationen genommen werden. Stehen diese zwar dem Entwickler zur Verfügung, tauchen aber nicht im Model auf, können diese nicht für den generativen Ansatz herangezogen werden.

Das vorgehen beim implementieren des Referenzsystems erfolgt so, dass zunächst innerhalb eines Services alle benötigten Testmethoden erstellt werden bis zu Fertigstellung. Sind diese Implementierten Tests erfolgreich fertig gestellt, werden diese im nächsten Service implementiert. Sobald an der Implementierung nur noch Namen ausgetauscht werden müssen ist die Grundlage geschaffen, um die vorgenommenen Änderungen am Referenzsystem in Barrakuda übernommen werden können.

% ----------------------------------------------------------------------------------------------------------
% Fazit
% ----------------------------------------------------------------------------------------------------------
\section{Fazit}\label{ch:fazit}

Insbesondere das component und \acf{CDCT} erweisen sich bei Microservice-Architekturen als sehr sinnvoll. Unit- und Integrationstests unterscheiden sich aufgrund der Größe ihres scopes in Sinn und Zweck nicht von der Verwendung in monolithischen Anwendungen. Aber bereits beim component testing ist es bei der Implementierung von sinnvollen Testfällen möglich bereits Abweichungen in der Definition von nach außen getragenen Schnittstellen zu erkennen. Und das bereits, bevor aufwändige test doubles oder gar ganze Infrastrukturen gestartet werden müssen.

Allerdings erweist sich \ac{CDCT} für den generativen Ansatz als nicht sehr hilfreich. Im Modell fehlen schlichtweg zu viele Informationen über die Kommunikationen die zwischen den Services stattfinden. Somit werden contracts generiert, die Schlichtweg alle möglichen Schnittstellen definieren, auch wenn diese eventuell später gar nicht als Kommunikationsweg genutzt werden. Sinnvoll sind diese allerdings immer noch dafür, die Verwendung von end-to-end tests einzuschränken. Die korrekte Funktionalität von Schnittstellen wird bereits in diesem Testschritt gewährleistet, sodass in den end-to-end tests wirklich nur noch das Zusammenspiel aller Services geprüft werden muss.

Der generative Ansatz wird in jedem Fall zu einer Zeitersparnis führen. Wie in den Abbildungen \ref{fig:images/img_coverage_ordering.jpg} und \ref{fig:images/img_coverage_shoppingcart.jpg} zu sehen ist, wird mit den Tests die generierbar sind bereits eine code coverage von 85\%, bzw. 87\% erreicht. Und das mit einer vergleichsweise geringen Menge an input-Daten aus dem Modell.

\image[1]{images/img_coverage_ordering.PNG}{Code coverage des Ordering-Service}
\image[1]{images/img_coverage_shoppingcart.PNG}{Code coverage des Shoppingcart-Service}

Als Erweiterung wäre es aber, wie bereits erwähnt, sinnvoll, dass Kommunikationsschnittstellen zwischen modellierten Services in das Modell mit aufzunehmen, um sinnvollere Testfälle insbesondere in den \ac{CDCT} und end-to-end tests generieren zu können.

Doch für einen höheren Informationsgehalt das Model von Barrakuda zu verkomplizieren ist eventuell auch keine gute Lösung, da insbesondere die schnelle und einfache Modellierung aktuell den Reiz der Anwendung ausmacht. Und Kommunikationswege anschaulich und überschaubar in Textform zu modellieren ist ebenfalls nicht leicht umzusetzen.

Zu einem Großteil der Zeitersparnis führt auch das Grundgerüst, was mit den im Rahmen dieser Arbeit entwickelten Tests bereitgestellt wird. Bei it@M wird gerade erst mit der Produktiven Verwendung von Microservice-Architekturen begonnen. Ein Großteil der Entwickler muss sich mit teilweise völlig neuen Frameworks auseinandersetzen, neue Denkweisen bei der Lösung von Problemen müssen bedacht werden. Sind in den generierten Systemen nun bereits grundlegende Tests implementiert, und die Strategien hinter diesen erläutert, erlaubt dies den Entwicklern sich auf das erstellen von Testfällen zu konzentrieren, ohne sich dabei zunächst Gedanken um die Verwendungen des Frameworks machen zu müssen oder sich mit Konfigurationen zu beschäftigen. Ein schneller Einstieg mit einer niedrigeren ersten Hürde zum implementieren von Tests kann die Motivation zum schreiben von diesen erhöhen.

Auch sorgt die Generierung für weitere Standardisierung in den Systemen. Wurden bereits Vorkehrungen im Generat getroffen, die das automatische Ausführen der Testmethoden ermöglichen, werden diese auch mit hoher Wahrscheinlichkeit genutzt werden, sodass der \acs{CI}-Prozess verbessert wird. Ebenso wie es bereits durch die Generierung von Docker-Files und dem damit entstehenden Interesse an Container-Lösungen der Fall war.

Abschließend lässt sich sagen, dass der generative Ansatz zur Testerstellung in Microservice-Architekturen in diesem Anwendungsfall sinnvoll ist. Dies liegt aber vor allem darin, dass bereits eine domänenspezifische Sprache, sowie der dazugehörige Generator vorliegt, und die im Rahmen dieser Arbeit erarbeiteten Testmethoden nur eine Erweiterung darstellen. Eine eigenständige Anwendung zu entwickeln, die für die reine Testgenerierung zuständig ist, ist sowohl unmöglich (aufrund der vielen verfügbaren Frameworks und Sprachen zur Umsetzung einer Microservice-Architektur) als auch zu Zeitaufwendig für einen Entwickler zu bediene. Währenddessen ist die Erweiterung von Barrakuda ein zusätzlicher Vorteil der für die Verwendung der Anwendung spricht und den Entwicklern der Anwendungen hoffentlich nicht nur zeitliche Vorteile verschaffen kann.

% ----------------------------------------------------------------------------------------------------------
% Literatur
% ----------------------------------------------------------------------------------------------------------
\renewcommand\refname{Quellenverzeichnis}
\bibliographystyle{settings/bibstyle}
\bibliography{settings/bibfile}
\pagebreak

% ----------------------------------------------------------------------------------------------------------
% Anhang
% ----------------------------------------------------------------------------------------------------------
\pagenumbering{Roman}
\setcounter{page}{1}
\lhead{Anhang \thesection}

\end{document}
